{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RM5t8ExtIdSd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "max_iters = 5000\n",
        "n_embd = 64\n",
        "dropout = 0.0\n",
        "block_size = 32\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "batch_size = 16\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "learning_rate = 1e-3\n",
        "eval_iters = 200\n",
        "eval_interval = 100\n",
        "\n",
        "# Data loading and preparation\n",
        "with open(\"all_tswift_lyrics.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[n] for n in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n_split = int(0.9 * len(data))\n",
        "train_data = data[:n_split]\n",
        "val_data = data[n_split:]\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
        "    y = torch.stack(\n",
        "        [data[i + 1 : i + block_size + 1] for i in ix]\n",
        "    )  # target is input shifted to right by one position\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_losses():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:  # get the mean of both train and eval loss\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class attention_head(nn.Module):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)  # just in case is necessary\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B, T, head_size)\n",
        "        q = self.query(x)  # (B, T, head_size)\n",
        "        v = self.value(x)  # (B, T, head_size)\n",
        "        wei = (\n",
        "            q @ k.transpose(-2, -1) * C**-0.5\n",
        "        )  # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "        tril = torch.tril(torch.ones(T, T))\n",
        "        wei = wei.masked_fill(\n",
        "            tril == 0, float(\"-inf\")\n",
        "        )  # wei can be interpreted as logits before applying softmax, if I want to mask out the future, I can do so by filling the future with -inf\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [attention_head(head_size) for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class attention_block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.n_head = n_head\n",
        "        self.head_size = n_embd // n_head\n",
        "        self.multi_head = MultiHeadAttention(n_head, self.head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.multi_head(\n",
        "            self.ln1(x)\n",
        "        )  # the '+' for residual connections, layer norm is applied before the multi head attention\n",
        "        x = x + self.ffwd(\n",
        "            self.ln2(x)\n",
        "        )  # the '+' for residual connections, layer norm is applied before the feed forward\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[attention_block(n_embd, n_head) for _ in range(n_layer)]\n",
        "        )\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
        "        self.lm_head = nn.Linear(\n",
        "            n_embd, vocab_size\n",
        "        )  # project the output of the final block to logits\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.embedding_table(idx)  # (B, T, n_embd)\n",
        "        pos_emb = self.positional_embedding_table(\n",
        "            torch.arange(T)\n",
        "        )  # (T, n_embd), it's like passing to pos_emb [0,1,...T]\n",
        "        x = tok_emb + pos_emb  # (B, T, n_embd) + (T, n_embd) -> (B, T, n_embd)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = (\n",
        "                logits.shape\n",
        "            )  # C is the vocab size aka number of classes for cross entropy loss\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)  # targets is (B, T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_tokens):\n",
        "        \"\"\"input idx is (B, T) array of indices in the current context\"\"\"\n",
        "        for _ in range(max_tokens):\n",
        "            # crop idx in order to fit in the model context length (block_size)\n",
        "            idx_cropped = idx[:, -block_size:]\n",
        "            # get logits and loss from the model\n",
        "            logits, loss = self(idx_cropped)\n",
        "            # focus only on the last logits aka the prediction for the next token\n",
        "            logits = logits[:, -1, :]  # becomes (B, vocab_size)\n",
        "            # apply softmax to get probabilities over possible next tokens\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "def train(model):\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # training loop\n",
        "    for iter in range(max_iters):\n",
        "\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = get_losses()\n",
        "            print(\n",
        "                f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
        "            )\n",
        "\n",
        "        # sample data\n",
        "        xb, yb = get_batch(\"train\")\n",
        "\n",
        "        # evaluate loss\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZxgcITbIs6O",
        "outputId": "4f4cdeb1-52f5-4514-d326-87ae502c5fcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 4.4868, val loss 4.4977\n",
            "step 100: train loss 2.5605, val loss 2.5820\n",
            "step 200: train loss 2.4090, val loss 2.4291\n",
            "step 300: train loss 2.3099, val loss 2.3485\n",
            "step 400: train loss 2.2243, val loss 2.2534\n",
            "step 500: train loss 2.1395, val loss 2.1836\n",
            "step 600: train loss 2.0621, val loss 2.1304\n",
            "step 700: train loss 1.9906, val loss 2.0689\n",
            "step 800: train loss 1.9605, val loss 2.0256\n",
            "step 900: train loss 1.9045, val loss 1.9713\n",
            "step 1000: train loss 1.8633, val loss 1.9475\n",
            "step 1100: train loss 1.8173, val loss 1.9013\n",
            "step 1200: train loss 1.7977, val loss 1.8952\n",
            "step 1300: train loss 1.7607, val loss 1.8616\n",
            "step 1400: train loss 1.7364, val loss 1.8485\n",
            "step 1500: train loss 1.7206, val loss 1.8396\n",
            "step 1600: train loss 1.6843, val loss 1.7998\n",
            "step 1700: train loss 1.6776, val loss 1.7982\n",
            "step 1800: train loss 1.6611, val loss 1.7855\n",
            "step 1900: train loss 1.6396, val loss 1.7611\n",
            "step 2000: train loss 1.6306, val loss 1.7610\n",
            "step 2100: train loss 1.6090, val loss 1.7589\n",
            "step 2200: train loss 1.6111, val loss 1.7265\n",
            "step 2300: train loss 1.5841, val loss 1.7199\n",
            "step 2400: train loss 1.5682, val loss 1.7323\n",
            "step 2500: train loss 1.5586, val loss 1.7118\n",
            "step 2600: train loss 1.5547, val loss 1.7063\n",
            "step 2700: train loss 1.5368, val loss 1.6997\n",
            "step 2800: train loss 1.5297, val loss 1.7082\n",
            "step 2900: train loss 1.5268, val loss 1.6986\n",
            "step 3000: train loss 1.5179, val loss 1.6904\n",
            "step 3100: train loss 1.5164, val loss 1.6987\n",
            "step 3200: train loss 1.4997, val loss 1.7042\n",
            "step 3300: train loss 1.4936, val loss 1.6897\n",
            "step 3400: train loss 1.4908, val loss 1.6804\n",
            "step 3500: train loss 1.4943, val loss 1.6891\n",
            "step 3600: train loss 1.4815, val loss 1.6812\n",
            "step 3700: train loss 1.4534, val loss 1.6810\n",
            "step 3800: train loss 1.4535, val loss 1.6782\n",
            "step 3900: train loss 1.4413, val loss 1.6498\n",
            "step 4000: train loss 1.4415, val loss 1.6486\n",
            "step 4100: train loss 1.4463, val loss 1.6458\n",
            "step 4200: train loss 1.4308, val loss 1.6648\n",
            "step 4300: train loss 1.4392, val loss 1.6650\n",
            "step 4400: train loss 1.4252, val loss 1.6406\n",
            "step 4500: train loss 1.4221, val loss 1.6578\n",
            "step 4600: train loss 1.4294, val loss 1.6619\n",
            "step 4700: train loss 1.4049, val loss 1.6456\n",
            "step 4800: train loss 1.3951, val loss 1.6513\n",
            "step 4900: train loss 1.3975, val loss 1.6463\n",
            "step 4999: train loss 1.4041, val loss 1.6534\n"
          ]
        }
      ],
      "source": [
        "model = GPT()\n",
        "model = model.to(device)\n",
        "train(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sL9419i1Je4a"
      },
      "outputs": [],
      "source": [
        "input_text = 'I'\n",
        "input_token = encode(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CmCIATJHL7OK"
      },
      "outputs": [],
      "source": [
        "input_tensor = torch.tensor(input_token, dtype=torch.long, device=device).view(1,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDF_p1JyMBCg",
        "outputId": "a074850d-3d70-482d-b13c-eec3f81f62b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I wross a thope ying..\n",
            "Your eyes\n",
            "And it's on doce that I'm cy tored\n",
            "I's headed it places\n",
            "And a big baby, nowly baby\n",
            "3:] you hope there are is stay, not the the door I in this screating her pocks arm\n",
            "Flew I had you style tell I thought me how pake\n",
            "Then I look to the care everything to wrong\n",
            "Got then tracks from through (bod with into fast\n",
            "I out wor the crazy that I was swandoffall tell masing, there watching,\n",
            "But you made many clesump in wors\n",
            "Trouble, there paint likering?\n",
            "Wham would your friend brod\n",
            "I neard from frissing it rains look it\n",
            "And the way it's not right silentipt off heart on your your me\n",
            "'Cause heahUre byerning now I would\n",
            "And me nobody why didning ready for am as heart\n",
            "And Speakers under listen\n",
            "\n",
            "Backmoriting down now\n",
            "You back times abovend with you, I need.\n",
            "\n",
            "And through people it's a from asn't, you think at I'd rather haunt it true as pay\n",
            "Living down from you hit cause \n",
            "And he's singing right drair, down fade it whas would you\n",
            "\n",
            "Who you say Eday I can't be by count around \n",
            "And my matte I'm mad blrood\n",
            "I come spermarthd\n",
            "And -'cape on my and, \"We go can't\n",
            "\n",
            "And it all my yeirour breakers\n",
            "Iblodmarin' believe with the rebeeper of your bare you free Up your ears\n",
            "Or dand sing him coat, talk\n",
            "I'm beday, and it come down\n",
            "Well made for a rabound and remember\n",
            "Ever mistracked to me talking hard\n",
            "Hope on a day, yecause don't head there's was and \"He wheer's would\n",
            "And I would wonderstrate arout\n",
            "If you're stoppin back too him no way, whittle, here my can hir crosed his tonight to me\n",
            "God get's guy from dimn with words,\n",
            "Dream how like you would\n",
            "\n",
            "But ill can't\n",
            "Better shame in a no\n",
            "\n",
            "Who doever, brom!, and i made and it's not hate\n",
            "Ell they's milles\n",
            "For Sworth sky, I'd you bet through it up could, nevertiming,\n",
            "And that 'cause Christmas\n",
            "Dreaming my sind you can see \n",
            "\n",
            "Breakin' not look and almone tonight\n",
            "And it pathers say it when I'm on losn see\n",
            "\n",
            "You this nexdam if I love the don't\n",
            "So wrondow you, when loss and when well\n",
            "Sue dourstrater\n",
            "Frighting to screept becdristmas just list\n"
          ]
        }
      ],
      "source": [
        "print(decode(model.generate(input_tensor, max_tokens=2000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
